{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Yousaf Khaliq, Danlei Zhu, Haoyuan Wang\n",
        "#We are using ChatGPT and Gemini 2.5 pro to help with coding."
      ],
      "metadata": {
        "id": "990Q3VpoD3Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Complete AI Research Assistant Code (RAG + TF-IDF Similarities)\n",
        "\n",
        "# --- 1. Install Dependencies ---\n",
        "!pip install -qU \\\n",
        "    arxiv \\\n",
        "    pypdf \\\n",
        "    langchain \\\n",
        "    -U langchain-community\\\n",
        "    openai \\\n",
        "    tiktoken \\\n",
        "    faiss-cpu \\\n",
        "    gradio \\\n",
        "    seaborn \\\n",
        "    matplotlib \\\n",
        "    scikit-learn \\\n",
        "    # aiohttp # Likely no longer needed as we won't re-download \\\n",
        "    # wordcloud # Not used in the provided similarity code snippet\n",
        "    # azure.identity # Not used in the provided similarity code snippet\n",
        "\n",
        "print(\"Dependencies installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FszvwjN-IaaL",
        "outputId": "3f00a429-a85b-414a-856b-986e4c442c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Dependencies installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Import Libraries and Set Up ---\n",
        "import os\n",
        "import shutil\n",
        "import arxiv\n",
        "import gradio as gr\n",
        "from getpass import getpass\n",
        "import datetime\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Any # For type hinting from new code\n",
        "\n",
        "# LangChain components\n",
        "from langchain.document_loaders import PyPDFLoader # Keep for RAG indexing\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Imports from the new similarity code\n",
        "from pypdf import PdfReader # For direct PDF reading for similarity text extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"Libraries imported.\")\n",
        "\n",
        "# --- OpenAI API Key Setup ---\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key (needed for RAG chat): \")\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    print(\"‚ö†Ô∏è OpenAI API Key not provided. RAG Chat features will NOT work.\")\n",
        "else:\n",
        "    print(\"OpenAI API Key set.\")\n",
        "\n",
        "# --- Constants and Directories ---\n",
        "PDF_FOLDER = \"arxiv_pdfs\"\n",
        "REF_FOLDER = \"arxiv_references\"\n",
        "VECTORSTORE_PATH = \"faiss_index\" # For RAG\n",
        "\n",
        "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
        "os.makedirs(REF_FOLDER, exist_ok=True)\n",
        "print(f\"Directories '{PDF_FOLDER}' and '{REF_FOLDER}' created/ensured.\")"
      ],
      "metadata": {
        "id": "pk3OjPwyLfWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d65f99-3112-4337-9405-48a5821bdf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n",
            "Enter your OpenAI API Key (needed for RAG chat): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "OpenAI API Key set.\n",
            "Directories 'arxiv_pdfs' and 'arxiv_references' created/ensured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Core Functions ---\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def clean_filename(title: str) -> str:\n",
        "    \"\"\"Removes invalid characters for filenames and limits length.\"\"\"\n",
        "    cleaned = re.sub(r'[\\\\/*?:\"<>|]', \"\", title) # Remove invalid chars\n",
        "    cleaned = re.sub(r'\\s+', '_', cleaned)       # Replace spaces with underscores\n",
        "    return cleaned[:100]                         # Truncate long filenames\n",
        "\n",
        "def format_reference(result: arxiv.Result) -> str:\n",
        "    \"\"\"Formats a reference string for an arXiv paper.\"\"\"\n",
        "    try:\n",
        "        authors = \", \".join(author.name for author in result.authors)\n",
        "        year = result.published.year\n",
        "        title = result.title.replace('\\n', '').replace('  ', ' ') # Clean title\n",
        "        arxiv_id = result.get_short_id()\n",
        "        pdf_link = result.pdf_url\n",
        "        return f\"{authors} ({year}). *{title}*. arXiv:{arxiv_id}. Available at: {pdf_link}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error formatting reference for {result.entry_id}: {e}\")\n",
        "        return f\"Error formatting reference for {result.entry_id}\"\n",
        "\n",
        "# --- Paper Fetching (Downloads PDFs needed by RAG & TF-IDF) ---\n",
        "def fetch_papers(topic: str, sort_by: str, max_results: int) -> (List[Dict[str, Any]], List[str], str | None, str | None):\n",
        "    \"\"\"Fetches papers from arXiv, downloads PDFs, creates references list, and returns detailed paper info.\"\"\"\n",
        "    print(f\"Fetching papers for topic: '{topic}', sorting by: {sort_by}, max results: {max_results}\")\n",
        "    # --- Cleanup ---\n",
        "    if os.path.exists(PDF_FOLDER): shutil.rmtree(PDF_FOLDER)\n",
        "    if os.path.exists(REF_FOLDER): shutil.rmtree(REF_FOLDER)\n",
        "    if os.path.exists(VECTORSTORE_PATH): shutil.rmtree(VECTORSTORE_PATH) # Clear RAG index\n",
        "    os.makedirs(PDF_FOLDER, exist_ok=True)\n",
        "    os.makedirs(REF_FOLDER, exist_ok=True)\n",
        "    print(\"Cleared previous downloads, references, and RAG index.\")\n",
        "\n",
        "    try:\n",
        "        # Configure arXiv search parameters\n",
        "        search_params = {\"query\": topic, \"max_results\": max_results}\n",
        "        if sort_by == \"Relevance\": search_params[\"sort_by\"] = arxiv.SortCriterion.Relevance\n",
        "        elif sort_by == \"Date\":\n",
        "             search_params[\"sort_by\"] = arxiv.SortCriterion.SubmittedDate\n",
        "             search_params[\"sort_order\"] = arxiv.SortOrder.Descending\n",
        "        client = arxiv.Client() # Instantiate client\n",
        "        search = arxiv.Search(**search_params)\n",
        "        # Use client.results for current arxiv library version\n",
        "        results = list(client.results(search)) # Execute search\n",
        "\n",
        "        if not results:\n",
        "            print(\"No papers found.\")\n",
        "            return [], [], None, None # Return empty lists and None for zip paths\n",
        "\n",
        "        print(f\"Found {len(results)} papers. Downloading...\")\n",
        "        # --- Process Results ---\n",
        "        paper_details = []         # List to store detailed dict for each paper\n",
        "        references_list = []       # List for the formatted references text file\n",
        "        ref_filename = f\"references_{clean_filename(topic)}_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.txt\"\n",
        "        ref_filepath = os.path.join(REF_FOLDER, ref_filename)\n",
        "\n",
        "        with open(ref_filepath, \"w\", encoding=\"utf-8\") as ref_file:\n",
        "            for i, result in enumerate(results):\n",
        "                progress = f\"[{i+1}/{len(results)}]\"\n",
        "                try:\n",
        "                    title = result.title.replace('\\n', '').replace('  ', ' ')\n",
        "                    filename_base = clean_filename(f\"{result.get_short_id()}_{title}\")\n",
        "                    pdf_filename = f\"{filename_base}.pdf\"\n",
        "                    pdf_filepath = os.path.join(PDF_FOLDER, pdf_filename) # Absolute path to downloaded PDF\n",
        "\n",
        "                    print(f\"{progress} Downloading '{title}' to {pdf_filename}...\")\n",
        "                    result.download_pdf(dirpath=PDF_FOLDER, filename=pdf_filename) # Download the PDF\n",
        "\n",
        "                    # Store detailed info needed by both RAG and TF-IDF parts\n",
        "                    paper_info = {\n",
        "                        \"index\": i + 1, # User-facing 1-based index\n",
        "                        \"title\": title,\n",
        "                        \"filename\": pdf_filename, # Just the filename\n",
        "                        \"filepath\": pdf_filepath, # Full path for loading\n",
        "                        \"arxiv_id\": result.get_short_id(),\n",
        "                        \"summary\": result.summary, # Needed for TF-IDF\n",
        "                        # Keep other potentially useful metadata\n",
        "                        \"url\": result.pdf_url,\n",
        "                        \"doi\": result.doi,\n",
        "                        \"published\": str(result.published),\n",
        "                        \"authors\": [a.name for a in result.authors],\n",
        "                        # Placeholders to be filled later\n",
        "                        \"clean_text\": \"\", # For TF-IDF full text\n",
        "                        # \"references\": [] # REMOVED - No longer extracting references for Jaccard\n",
        "                    }\n",
        "                    paper_details.append(paper_info)\n",
        "\n",
        "                    # Add formatted reference to list and file\n",
        "                    ref_string = format_reference(result)\n",
        "                    references_list.append(ref_string)\n",
        "                    ref_file.write(ref_string + \"\\n\\n\")\n",
        "                    print(f\"{progress} Download complete.\")\n",
        "\n",
        "                except Exception as e: # Catch download/processing errors\n",
        "                    print(f\"{progress} ‚ö†Ô∏è Failed to download/process paper {result.entry_id} ('{result.title}'): {type(e).__name__} - {e}\")\n",
        "                    # Attempt to add a placeholder reference even on failure\n",
        "                    try:\n",
        "                        ref_string = format_reference(result)\n",
        "                        references_list.append(f\"[Download Failed] {ref_string}\")\n",
        "                        ref_file.write(f\"[Download Failed] {ref_string}\\n\\n\")\n",
        "                    except Exception as ref_e: print(f\"Could not format reference for failed paper {result.entry_id}: {ref_e}\")\n",
        "\n",
        "        print(f\"Downloads finished. References saved in '{ref_filepath}'.\")\n",
        "        # Create zip archives for download buttons\n",
        "        pdf_zip_path = shutil.make_archive(f\"{PDF_FOLDER}\", 'zip', PDF_FOLDER)\n",
        "        ref_zip_path = shutil.make_archive(f\"{REF_FOLDER}\", 'zip', REF_FOLDER)\n",
        "        return paper_details, references_list, pdf_zip_path, ref_zip_path\n",
        "\n",
        "    except Exception as e: # Catch broader errors during fetching\n",
        "        print(f\"‚ùå An error occurred during arXiv fetching: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return [], [], None, None\n",
        "\n",
        "\n",
        "# --- RAG Core: Indexing Function (Uses OpenAI Embeddings) ---\n",
        "def load_and_index_papers_for_rag(paper_details: List[Dict[str, Any]]) -> FAISS | None:\n",
        "    \"\"\"Loads PDFs using Langchain's PyPDFLoader, splits text, creates OpenAI embeddings, and builds FAISS index for RAG chat.\"\"\"\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "         print(\"‚ö†Ô∏è Cannot index papers for RAG: OpenAI API Key not set.\")\n",
        "         return None\n",
        "\n",
        "    # Filter details for papers that were successfully downloaded\n",
        "    valid_paper_details = [p for p in paper_details if os.path.exists(p['filepath'])]\n",
        "    if not valid_paper_details:\n",
        "        print(f\"‚ö†Ô∏è No valid PDF files found for RAG indexing based on fetched details.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loading PDFs for RAG indexing (using Langchain PyPDFLoader)...\")\n",
        "    all_docs_for_rag = []\n",
        "    docs_loaded_count = 0\n",
        "    # Load docs and add metadata needed for RAG source attribution\n",
        "    for details in valid_paper_details:\n",
        "        filepath = details['filepath']\n",
        "        # --- ADDED LOGGING ---\n",
        "        print(f\" -> RAG Indexing: Attempting to load Paper {details['index']} ('{details['title'][:40]}...') from {details['filename']}\")\n",
        "        try:\n",
        "            loader = PyPDFLoader(filepath) # Langchain loader\n",
        "            docs = loader.load()\n",
        "            if not docs:\n",
        "                 # --- ADDED LOGGING ---\n",
        "                 print(f\"    -> WARNING: No documents loaded by PyPDFLoader for Paper {details['index']}. PDF might be empty or unreadable by this loader.\")\n",
        "                 continue # Skip if loader returns nothing\n",
        "\n",
        "            # --- ADDED LOGGING ---\n",
        "            print(f\"    -> Loaded {len(docs)} pages for Paper {details['index']}.\")\n",
        "            docs_loaded_count += 1\n",
        "            for doc in docs: # Add metadata to each page/chunk\n",
        "                doc.metadata['source'] = filepath # Essential for filtering\n",
        "                doc.metadata['filename'] = details['filename']\n",
        "                doc.metadata['title'] = details['title']\n",
        "                doc.metadata['paper_index'] = details['index']\n",
        "                # PyPDFLoader adds 'page' (0-indexed) automatically\n",
        "                if 'page' not in doc.metadata: # Defensive check\n",
        "                     print(f\"   -> WARNING: 'page' metadata missing in loaded doc for {details['filename']}\")\n",
        "                     doc.metadata['page'] = -1 # Assign default if missing\n",
        "            all_docs_for_rag.extend(docs)\n",
        "        except Exception as e:\n",
        "             # --- ADDED LOGGING ---\n",
        "            print(f\"    -> ERROR loading Paper {details['index']} with PyPDFLoader: {type(e).__name__} - {e}. Skipping for RAG.\")\n",
        "\n",
        "    if not all_docs_for_rag:\n",
        "        print(\"‚ùå No documents could be loaded by Langchain loader for RAG indexing.\")\n",
        "        return None\n",
        "    print(f\"RAG Indexing: Successfully loaded pages from {docs_loaded_count}/{len(valid_paper_details)} valid PDFs using PyPDFLoader.\")\n",
        "\n",
        "    print(f\"RAG Indexing: Splitting text for {len(all_docs_for_rag)} pages...\")\n",
        "    # Use RecursiveCharacterTextSplitter for robust splitting\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "    texts_for_rag = text_splitter.split_documents(all_docs_for_rag)\n",
        "\n",
        "    if not texts_for_rag:\n",
        "        print(\"‚ö†Ô∏è RAG Indexing: No text chunks generated after splitting. Check PDF text content and splitter settings.\")\n",
        "        return None\n",
        "\n",
        "    # --- ADDED LOGGING: Chunks per paper ---\n",
        "    chunks_per_paper = {}\n",
        "    for chunk in texts_for_rag:\n",
        "        p_index = chunk.metadata.get('paper_index', 'Unknown')\n",
        "        chunks_per_paper[p_index] = chunks_per_paper.get(p_index, 0) + 1\n",
        "    print(\"RAG Indexing: Chunks created per paper index:\")\n",
        "    if chunks_per_paper:\n",
        "         for p_index, count in sorted(chunks_per_paper.items()):\n",
        "              print(f\"  - Paper {p_index}: {count} chunks\")\n",
        "    else:\n",
        "         print(\"  -> No chunks were associated with paper indices (check metadata addition).\")\n",
        "    # --- END LOGGING ---\n",
        "\n",
        "    print(f\"RAG Indexing: Split into {len(texts_for_rag)} total chunks. Creating OpenAI embeddings...\")\n",
        "    try:\n",
        "        embeddings = OpenAIEmbeddings() # Uses the API key from environment\n",
        "        print(\"RAG Indexing: Initializing FAISS vector store...\")\n",
        "        vector_store = FAISS.from_documents(texts_for_rag, embeddings) # Create index from docs & embeddings\n",
        "        # Optional: vector_store.save_local(VECTORSTORE_PATH)\n",
        "        print(f\"RAG Indexing: FAISS index created successfully with {vector_store.index.ntotal} vectors.\")\n",
        "        return vector_store\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå RAG Indexing: Error creating OpenAI embeddings or FAISS index: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- RAG Core: LLM and Prompt Setup ---\n",
        "def setup_llm_and_prompt() -> (ChatOpenAI | None, str | None):\n",
        "    \"\"\"Sets up the Langchain ChatOpenAI LLM and the QA prompt template string.\"\"\"\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "         print(\"‚ö†Ô∏è Cannot set up LLM: OpenAI API Key not set.\")\n",
        "         return None, None\n",
        "    print(\"Setting up LLM (gpt-3.5-turbo) and QA Prompt Template String...\")\n",
        "    try:\n",
        "        # Initialize the LLM\n",
        "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1, streaming=False) # Low temp for factual RAG\n",
        "\n",
        "        # Define the detailed prompt template as a string - Includes post-context instruction\n",
        "        prompt_template_string = \"\"\"You are an AI research assistant analyzing academic papers. Use the following pieces of context, which are excerpts from specific research papers identified by their metadata (title, page number), to answer the question accurately and concisely.\n",
        "\n",
        "        **Instructions:**\n",
        "        1.  Base your answer *exclusively* on the provided context below.\n",
        "        2.  If the context does not contain the information needed to answer the question, explicitly state that the information is not available in the provided excerpts.\n",
        "        3.  Do not introduce outside knowledge or make assumptions beyond the text given.\n",
        "        4.  When asked to compare papers, clearly attribute points to the specific paper they came from, based *only* on the context provided for each. Use clear headings or bullet points for each paper. Double-check your attribution.\n",
        "        5.  Structure your answer clearly. For summaries, provide the key points. For comparisons, highlight similarities and differences methodically.\n",
        "\n",
        "        Context:\n",
        "        ---------\n",
        "        {context}\n",
        "        ---------\n",
        "\n",
        "        **Based strictly on the context above, answer the following question:**\n",
        "        Question: {question}\n",
        "\n",
        "        Answer:\"\"\"\n",
        "        print(\"LLM and Prompt Template String setup complete.\")\n",
        "        # Return the LLM instance and the template *string*\n",
        "        return llm, prompt_template_string\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error setting up LLM or Prompt String: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# --- TF-IDF Similarity Calculation Functions ---\n",
        "\n",
        "def clean_extracted_text_for_similarity(text: str, min_line_length: int = 20) -> str:\n",
        "    \"\"\"Clean extracted PDF text for TF-IDF analysis.\"\"\"\n",
        "    patterns = [ # Regex patterns from the provided code...\n",
        "        r'arXiv:\\d+\\.\\d+v\\d+', r'DOI:\\s?[\\w./-]+', r'Proceedings of .*?\\d{4}', r'Page\\s\\d+',\n",
        "        r'Figure\\s\\d+[:.]?', r'Table\\s\\d+[:.]?', r'\\$[^$]+\\$', r'\\\\begin\\{equation\\}.*?\\\\end\\{equation\\}',\n",
        "        r'¬©\\d{4}\\s(?:IEEE|ACM|Springer|Elsevier)', r'http[s]?://\\S+', r'\\b(?:figure|table|equation)\\s*\\d+\\b',\n",
        "        r'\\b\\d+%\\b', r'\\b\\w{1,2}\\b', # Remove very short words\n",
        "    ]\n",
        "    cleaned = text\n",
        "    for pattern in patterns:\n",
        "        cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
        "\n",
        "    lines = []\n",
        "    for line in cleaned.split('\\n'):\n",
        "        line_strip = line.strip()\n",
        "        # Keep lines if they are reasonably long OR contain reference keywords\n",
        "        if len(line_strip) >= min_line_length or re.search(r'\\b(references|bibliography)\\b', line_strip, re.IGNORECASE):\n",
        "            lines.append(line_strip)\n",
        "\n",
        "    final_text = '\\n'.join(lines)\n",
        "    final_text = re.sub(r'\\n{3,}', '\\n\\n', final_text) # Condense blank lines\n",
        "    final_text = re.sub(r'\\s{2,}', ' ', final_text)   # Condense spaces within lines\n",
        "    return final_text\n",
        "\n",
        "def extract_full_text_for_similarity(pdf_filepath: str) -> str:\n",
        "    \"\"\"Extracts and cleans text from a local PDF file using pypdf for similarity analysis.\"\"\"\n",
        "    if not os.path.exists(pdf_filepath):\n",
        "        print(f\"Error (TF-IDF Text Extraction): PDF file not found at {pdf_filepath}\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_filepath)\n",
        "        raw_text = []\n",
        "        for i, page in enumerate(reader.pages):\n",
        "            try:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text: raw_text.append(page_text)\n",
        "            except Exception as page_e: # Catch errors on specific pages\n",
        "                print(f\"  - Warning (TF-IDF): Could not extract text from page {i+1} in {os.path.basename(pdf_filepath)}: {page_e}\")\n",
        "        if not raw_text:\n",
        "            print(f\"Warning (TF-IDF): No text extracted from PDF: {os.path.basename(pdf_filepath)}\")\n",
        "            return \"\"\n",
        "\n",
        "        full_text = \" \".join(raw_text)\n",
        "        cleaned_text = clean_extracted_text_for_similarity(full_text)\n",
        "        # Return only if substantial text remains after cleaning\n",
        "        return cleaned_text if len(cleaned_text.split()) > 50 else \"\"\n",
        "    except Exception as e: # Catch errors reading the whole PDF file\n",
        "        print(f\"Error (TF-IDF): Reading PDF {os.path.basename(pdf_filepath)}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def fit_tfidf_vectorizer(papers_with_text: List[Dict[str, Any]]) -> TfidfVectorizer | None:\n",
        "    \"\"\"Creates and fits a TF-IDF vectorizer.\"\"\"\n",
        "    print(\"Fitting TF-IDF Vectorizer...\")\n",
        "    # Combine summary and cleaned full text for fitting the vocabulary\n",
        "    all_texts = [p[\"summary\"] + \" \" + p.get(\"clean_text\", \"\") for p in papers_with_text if p.get(\"clean_text\")]\n",
        "    if not all_texts:\n",
        "         print(\"Error (TF-IDF): No text available to fit vectorizer.\")\n",
        "         return None\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words='english', max_df=0.6, min_df=2, # Filter very common/rare terms\n",
        "        sublinear_tf=True, norm='l2', ngram_range=(1, 2), # Use unigrams and bigrams\n",
        "        token_pattern=r'(?u)\\b[a-zA-Z]{3,}\\b', max_features=10000 # Keep words >= 3 letters\n",
        "    )\n",
        "    try:\n",
        "        vectorizer.fit(all_texts)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        print(f\"TF-IDF Vocabulary size: {len(feature_names)}\")\n",
        "        return vectorizer\n",
        "    except ValueError as ve: # Catch potential errors if vocabulary is empty after filtering\n",
        "         print(f\"Error (TF-IDF): Fitting vectorizer failed, likely due to insufficient valid terms. {ve}\")\n",
        "         return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error (TF-IDF): Fitting vectorizer failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- REMOVED Reference Extraction/Processing Functions ---\n",
        "\n",
        "def calculate_similarity_matrices(papers_with_text: List[Dict[str, Any]], vectorizer: TfidfVectorizer | None):\n",
        "    \"\"\"Calculate ONLY the TF-IDF Paper-to-Paper similarity matrix.\"\"\"\n",
        "    print(\"Calculating TF-IDF Paper-to-Paper Similarity Matrix...\") # Updated print\n",
        "    if not vectorizer:\n",
        "        print(\"Error (TF-IDF): Vectorizer not available.\")\n",
        "        return None, None # Return None for papers list and matrix\n",
        "\n",
        "    # Filter papers that have substantial cleaned text for TF-IDF calculations\n",
        "    valid_papers_for_tfidf = [p for p in papers_with_text if p.get(\"clean_text\")]\n",
        "    if len(valid_papers_for_tfidf) < 2:\n",
        "        print(\"Warning (TF-IDF): Need at least 2 papers with extracted text for similarity calculation.\")\n",
        "        return None, None # Return None for papers list as well\n",
        "\n",
        "    full_sim_matrix = None\n",
        "\n",
        "    # --- Paper-to-Paper Similarity (TF-IDF on Summary + Full Text) ---\n",
        "    combined_texts = [p[\"summary\"] + \" \" + p.get(\"clean_text\", \"\") for p in valid_papers_for_tfidf]\n",
        "    try:\n",
        "        combined_vectors = vectorizer.transform(combined_texts)\n",
        "        full_sim_matrix = cosine_similarity(combined_vectors)\n",
        "        print(\" -> Calculated Paper-to-Paper Similarity Matrix (TF-IDF).\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating paper-to-paper TF-IDF similarity: {e}\")\n",
        "        full_sim_matrix = None # Ensure it's None on error\n",
        "\n",
        "    # --- Shared References Section REMOVED ---\n",
        "\n",
        "    # Return the papers actually used for calculations, and ONLY the TF-IDF matrix\n",
        "    return valid_papers_for_tfidf, full_sim_matrix\n",
        "\n",
        "\n",
        "# --- Visualization Functions ---\n",
        "def plot_similarity_heatmap(matrix, labels, title) -> plt.Figure | None:\n",
        "    \"\"\"Plots similarity heatmap and returns the Matplotlib Figure. Returns None on error.\"\"\"\n",
        "    # Basic validation\n",
        "    if matrix is None or labels is None or len(labels) == 0 or matrix.shape[0] != len(labels) or matrix.shape[1] != len(labels):\n",
        "        print(f\"‚ö†Ô∏è Cannot plot heatmap '{title}': Invalid data provided.\")\n",
        "        fig, ax = plt.subplots(); ax.text(0.5, 0.5, f'Data unavailable for\\n{title}', ha='center', va='center', transform=ax.transAxes, fontsize=10, wrap=True); ax.set_xticks([]); ax.set_yticks([]); return fig\n",
        "\n",
        "    print(f\"Generating heatmap: {title}\")\n",
        "    try:\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "        fig_width = max(8, len(labels) * 0.8); fig_height = max(6, len(labels) * 0.7)\n",
        "        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
        "        short_labels = [f\"P{i+1}\" for i in range(len(labels))] # Use P1, P2...\n",
        "        annot = len(labels) <= 12\n",
        "        sns.heatmap(matrix, annot=annot, fmt=\".2f\", cmap=\"YlGnBu\", vmin=0, vmax=1, xticklabels=short_labels, yticklabels=short_labels, ax=ax, linewidths=0.5, linecolor='gray', annot_kws={\"size\": 9})\n",
        "        ax.set_title(title, fontsize=14, pad=15)\n",
        "        plt.xticks(rotation=45, ha=\"right\", fontsize=9); plt.yticks(rotation=0, fontsize=9)\n",
        "        plt.tight_layout(pad=1.5); plt.close(fig) # Prevent inline display\n",
        "        return fig\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating heatmap '{title}': {e}\")\n",
        "        fig, ax = plt.subplots(); ax.text(0.5, 0.5, f'Error plotting\\n{title}', ha='center', va='center', transform=ax.transAxes); ax.set_xticks([]); ax.set_yticks([]); return fig\n",
        "\n",
        "# --- REMOVED plot_shared_reference_heatmap function ---\n",
        "\n",
        "\n",
        "# --- 4. Gradio Web Application ---\n",
        "\n",
        "# --- State Management ---\n",
        "initial_state = {\n",
        "    # RAG Components\n",
        "    \"vector_store\": None, \"llm\": None, \"prompt_template_string\": None,\n",
        "    # Fetched Data\n",
        "    \"paper_details\": [], \"references_list\": [],\n",
        "    \"pdf_zip_path\": None, \"ref_zip_path\": None,\n",
        "}\n",
        "\n",
        "# --- Utility function for parsing paper references in queries ---\n",
        "def parse_paper_references(query: str, paper_details: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Parses user query to find references like 'paper 1' or 'paper titled \"X\"'.\"\"\"\n",
        "    target_details = []\n",
        "    # Look for 'paper <number>'\n",
        "    index_matches = re.findall(r'paper\\s+(\\d+)', query, re.IGNORECASE)\n",
        "    for index_str in index_matches:\n",
        "        try:\n",
        "            index = int(index_str)\n",
        "            found = next((p for p in paper_details if p['index'] == index), None)\n",
        "            if found: target_details.append(found)\n",
        "        except ValueError: continue\n",
        "\n",
        "    # Look for 'paper titled \"...\"'\n",
        "    title_matches = re.findall(r'paper titled\\s+[\"\\']([^\"\\']+)[\"\\']', query, re.IGNORECASE)\n",
        "    for title in title_matches:\n",
        "        normalized_title = title.strip().lower()\n",
        "        found = next((p for p in paper_details if p['title'].strip().lower() == normalized_title), None)\n",
        "        if found and found not in target_details: target_details.append(found) # Add if new\n",
        "\n",
        "    # Fallback: Check for direct title mentions if no specific pattern matched\n",
        "    if not target_details:\n",
        "        query_lower = query.lower()\n",
        "        for p in paper_details:\n",
        "             if p['title'].lower() in query_lower:\n",
        "                  is_substring = any(p['title'].lower() in existing['title'].lower() or existing['title'].lower() in p['title'].lower() for existing in target_details)\n",
        "                  if not is_substring and p not in target_details: target_details.append(p)\n",
        "\n",
        "    # Return unique identified papers\n",
        "    unique_targets = []; seen_indices = set()\n",
        "    for p in target_details:\n",
        "        if p['index'] not in seen_indices: unique_targets.append(p); seen_indices.add(p['index'])\n",
        "\n",
        "    if unique_targets: print(f\"Parsed query, identified target paper(s): {[p['index'] for p in unique_targets]}\")\n",
        "    else: print(\"Parsed query, no specific paper references identified.\")\n",
        "    return unique_targets\n",
        "\n",
        "\n",
        "# --- Gradio Interface Definition ---\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"indigo\", secondary_hue=\"purple\")) as demo:\n",
        "    app_state = gr.State(initial_state) # Holds the application's state\n",
        "\n",
        "    gr.Markdown(\"# ü§ñ AI Research Assistant (RAG Chat + TF-IDF Similarity)\") # Updated title\n",
        "    gr.Markdown(\"Fetches research papers from arXiv, processes them for RAG chat and TF-IDF similarity analysis.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # --- Left Column: Controls ---\n",
        "        with gr.Column(scale=1, min_width=300):\n",
        "            topic_input = gr.Textbox(label=\"Research Topic\", placeholder=\"e.g., Diffusion Models for Image Generation\")\n",
        "            sort_by_input = gr.Radio(label=\"Sort Papers By\", choices=[\"Relevance\", \"Date\"], value=\"Relevance\")\n",
        "            max_results_input = gr.Slider(label=\"Number of Papers\", minimum=1, maximum=25, value=5, step=1)\n",
        "            fetch_button = gr.Button(\"üîç Fetch & Process Papers\", variant=\"primary\")\n",
        "            status_output = gr.Markdown(value=\"*Status: Waiting for input...*\")\n",
        "\n",
        "        # --- Right Column: Paper List & Downloads ---\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### Fetched Papers List\")\n",
        "            papers_output = gr.Textbox(label=\"Paper Index & Title (Scrollable)\", lines=10, interactive=False, max_lines=15)\n",
        "            with gr.Row():\n",
        "                 download_pdfs_button = gr.DownloadButton(label=\"‚¨áÔ∏è Download PDFs (.zip)\", visible=False, size=\"sm\")\n",
        "                 download_refs_button = gr.DownloadButton(label=\"‚¨áÔ∏è Download References (.zip)\", visible=False, size=\"sm\")\n",
        "\n",
        "    gr.Markdown(\"---\") # Separator\n",
        "\n",
        "    # --- Similarity Visualizations Accordion (MODIFIED) ---\n",
        "    with gr.Accordion(\"üìä Similarity Analysis (TF-IDF)\", open=False): # Updated title\n",
        "        gr.Markdown(\"Visualization based on TF-IDF analysis of paper text (summary + full text). Uses P1, P2... labels; cross-reference with the list above.\")\n",
        "        with gr.Row(): # Only one plot now\n",
        "            tfidf_plot_output = gr.Plot(label=\"Paper-to-Paper Similarity (TF-IDF)\")\n",
        "            # --- REMOVED shared_ref_plot_output ---\n",
        "        # similarity_legend_output = gr.Markdown(value=\"\") # Placeholder for legend if needed\n",
        "\n",
        "    gr.Markdown(\"---\") # Separator\n",
        "\n",
        "    # --- Chat Interface ---\n",
        "    gr.Markdown(\"### üí¨ Chat with the Papers (RAG - OpenAI Powered)\")\n",
        "    gr.Markdown(\"Ask questions about the papers using index (`summarize paper 1`) or title (`compare paper titled 'X'...`).\")\n",
        "    chatbot_output = gr.Chatbot(label=\"Conversation\", height=450, bubble_full_width=False, avatar_images=None)\n",
        "    with gr.Row():\n",
        "        question_input = gr.Textbox(label=\"Your Question\", placeholder=\"Ask about the papers...\", scale=4, show_label=False)\n",
        "        ask_button = gr.Button(\"üí¨ Ask\", scale=1, variant=\"secondary\")\n",
        "        clear_chat_button = gr.Button(\"üßπ Clear\", scale=1)\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "\n",
        "    # 1. Fetch Button Click Logic (Orchestrates Fetching, RAG Indexing, TF-IDF Processing)\n",
        "    def fetch_and_process_papers(topic: str, sort_by: str, max_results: int, current_app_state: Dict[str, Any]):\n",
        "        \"\"\"Handles fetching, RAG setup, TF-IDF processing, and plot generation.\"\"\"\n",
        "        if not topic.strip(): # Handle empty topic\n",
        "            # Reset necessary UI elements\n",
        "            return { status_output: gr.update(value=\"*Status: Please enter a research topic.*\"), papers_output: gr.update(value=\"\"), download_pdfs_button: gr.update(visible=False), download_refs_button: gr.update(visible=False), chatbot_output: gr.update(value=[]), tfidf_plot_output: gr.update(value=None), # shared_ref_plot_output removed\n",
        "                     app_state: current_app_state }\n",
        "\n",
        "        # Initial status update and disabling UI elements\n",
        "        yield { status_output: gr.update(value=\"*Status: Fetching papers from arXiv...*\"), fetch_button: gr.update(interactive=False), chatbot_output: gr.update(value=[]), tfidf_plot_output: gr.update(value=None), # shared_ref_plot_output removed\n",
        "               }\n",
        "\n",
        "        # --- Step 1: Fetch Papers & Basic Details ---\n",
        "        new_state = initial_state.copy() # Reset state for new fetch\n",
        "        paper_details, references_list, pdf_zip, ref_zip = fetch_papers(topic, sort_by, max_results)\n",
        "        new_state[\"paper_details\"] = paper_details # Includes placeholders\n",
        "        new_state[\"references_list\"] = references_list\n",
        "        new_state[\"pdf_zip_path\"] = pdf_zip\n",
        "        new_state[\"ref_zip_path\"] = ref_zip\n",
        "\n",
        "        if not paper_details: # Handle case where no papers were found/downloaded\n",
        "             yield { status_output: gr.update(value=\"*Status: No papers found or download failed.*\"), papers_output: gr.update(value=\"No papers found.\"), download_pdfs_button: gr.update(visible=False), download_refs_button: gr.update(visible=False), fetch_button: gr.update(interactive=True), app_state: new_state }; return\n",
        "\n",
        "        # --- Display Fetched Paper List ---\n",
        "        papers_display_text = \"\\n\".join([f\"{p['index']}. {p['title']}\" for p in paper_details])\n",
        "        yield { status_output: gr.update(value=\"*Status: Papers fetched. Processing for RAG Chat...*\"), papers_output: gr.update(value=papers_display_text), download_pdfs_button: gr.update(value=pdf_zip, visible=bool(pdf_zip)), download_refs_button: gr.update(value=ref_zip, visible=bool(ref_zip)), app_state: new_state }\n",
        "\n",
        "        # --- Step 2: RAG Indexing (Uses OpenAI Embeddings) ---\n",
        "        vector_store = load_and_index_papers_for_rag(paper_details)\n",
        "        llm, prompt_template_str = None, None\n",
        "        if vector_store:\n",
        "            new_state[\"vector_store\"] = vector_store\n",
        "            yield { status_output: gr.update(value=\"*Status: RAG indexing complete. Setting up LLM...*\"), app_state: new_state }\n",
        "            llm, prompt_template_str = setup_llm_and_prompt()\n",
        "            if llm and prompt_template_str:\n",
        "                new_state[\"llm\"] = llm; new_state[\"prompt_template_string\"] = prompt_template_str\n",
        "                yield { status_output: gr.update(value=\"*Status: RAG setup complete. Processing for TF-IDF Similarity...*\"), app_state: new_state }\n",
        "            else: yield { status_output: gr.update(value=\"*Status: RAG indexing OK, but LLM setup failed. Chat disabled.*\"), app_state: new_state }\n",
        "        else: yield { status_output: gr.update(value=\"*Status: RAG indexing failed. Chat disabled. Processing for TF-IDF Similarity...*\"), app_state: new_state }\n",
        "\n",
        "        # --- Step 3: TF-IDF Similarity Processing ---\n",
        "        yield { status_output: gr.update(value=\"*Status: Extracting text & calculating TF-IDF similarity... (May take time)*\") }\n",
        "        processed_paper_details_for_tfidf = new_state[\"paper_details\"] # Modify the list in state\n",
        "        print(\"TF-IDF Phase: Extracting full text...\")\n",
        "        successful_extractions = 0\n",
        "        for paper in processed_paper_details_for_tfidf: # Modify list in place\n",
        "             if os.path.exists(paper['filepath']):\n",
        "                  paper['clean_text'] = extract_full_text_for_similarity(paper['filepath'])\n",
        "                  if paper['clean_text']: successful_extractions += 1\n",
        "                  else: print(f\"    -> Warning: No substantial text extracted for TF-IDF from Paper {paper['index']}\")\n",
        "             else: paper['clean_text'] = \"\"\n",
        "        print(f\"TF-IDF Phase: Successfully extracted text from {successful_extractions}/{len(processed_paper_details_for_tfidf)} papers.\")\n",
        "\n",
        "        # Fit vectorizer only if text was extracted\n",
        "        tfidf_vectorizer = None\n",
        "        if successful_extractions > 0:\n",
        "             tfidf_vectorizer = fit_tfidf_vectorizer(processed_paper_details_for_tfidf)\n",
        "\n",
        "        # Calculate matrix (MODIFIED Call)\n",
        "        tfidf_papers, full_sim_matrix = None, None # Initialize\n",
        "        if tfidf_vectorizer:\n",
        "             # *** ADJUSTED CALL: Only expect 2 return values ***\n",
        "             tfidf_papers, full_sim_matrix = calculate_similarity_matrices(\n",
        "                 processed_paper_details_for_tfidf, tfidf_vectorizer\n",
        "             )\n",
        "\n",
        "        # Generate Plots (MODIFIED)\n",
        "        tfidf_plot_fig = None\n",
        "        # --- REMOVED shared_ref_plot_fig variable ---\n",
        "\n",
        "        if tfidf_papers: # Check if calculate_similarity_matrices returned valid papers\n",
        "             paper_titles_for_plot = [p['title'] for p in tfidf_papers] # Use titles from the returned list\n",
        "             tfidf_plot_fig = plot_similarity_heatmap(full_sim_matrix, paper_titles_for_plot, \"Paper-to-Paper Similarity (TF-IDF)\")\n",
        "             # --- REMOVED call to plot_shared_reference_heatmap ---\n",
        "        else: print(\"Skipping TF-IDF plot generation as insufficient data was processed.\")\n",
        "\n",
        "        # --- Final Status Update (MODIFIED) ---\n",
        "        final_status = \"*Status: Ready! \"\n",
        "        if new_state.get(\"llm\"): final_status += \"RAG Chat active. \"\n",
        "        else: final_status += \"RAG Chat disabled. \"\n",
        "        # *** ADJUSTED check ***\n",
        "        if tfidf_plot_fig: final_status += \"TF-IDF Similarity plot generated.\"\n",
        "        else: final_status += \"Similarity analysis skipped or failed.\"\n",
        "\n",
        "        # MODIFIED final yield - removed shared_ref_plot_output\n",
        "        yield {\n",
        "            status_output: gr.update(value=final_status),\n",
        "            tfidf_plot_output: gr.update(value=tfidf_plot_fig),\n",
        "            fetch_button: gr.update(interactive=True), # Re-enable fetch button\n",
        "            app_state: new_state # Store final state\n",
        "        }\n",
        "\n",
        "    # MODIFIED click handler outputs - removed shared_ref_plot_output\n",
        "    fetch_button.click(\n",
        "        fetch_and_process_papers,\n",
        "        inputs=[topic_input, sort_by_input, max_results_input, app_state],\n",
        "        outputs=[status_output, papers_output, download_pdfs_button, download_refs_button,\n",
        "                 chatbot_output, tfidf_plot_output, # REMOVED shared_ref_plot_output\n",
        "                 fetch_button, app_state]\n",
        "    )\n",
        "\n",
        "\n",
        "    # 2. Ask Button Click Logic (Chat Handling - UPDATED Question for Summaries)\n",
        "    def handle_chat_message(question: str, history: List[List[str]], current_app_state: Dict[str, Any]):\n",
        "        vs = current_app_state.get(\"vector_store\")\n",
        "        llm = current_app_state.get(\"llm\")\n",
        "        prompt_template_str = current_app_state.get(\"prompt_template_string\")\n",
        "        paper_details = current_app_state.get(\"paper_details\", [])\n",
        "\n",
        "        # --- Pre-computation & RAG Readiness Checks ---\n",
        "        if not question.strip(): yield history, history, current_app_state; return\n",
        "        if not (vs and llm and prompt_template_str):\n",
        "             missing = [\"Vector Store\" if not vs else None, \"LLM\" if not llm else None, \"QA Prompt Template\" if not prompt_template_str else None]\n",
        "             error_msg = f\"‚ö†Ô∏è RAG system not ready. Missing: {', '.join(filter(None, missing))}. Please fetch/process papers first.\"\n",
        "             print(f\"ERROR in handle_chat: {error_msg}\"); history.append((question, error_msg)); yield history, history, current_app_state; return\n",
        "\n",
        "        history.append((question, None)); yield history, history, current_app_state # Add placeholder\n",
        "\n",
        "        # --- Parse Query & Retrieve Documents for RAG ---\n",
        "        target_paper_details = parse_paper_references(question, paper_details)\n",
        "        relevant_docs = []\n",
        "        retrieval_k = 5\n",
        "        retrieval_successful = False\n",
        "        retrieval_failure_reason = \"\"\n",
        "        try:\n",
        "            # [Keep the robust retrieval logic with title/question/generic fallback]\n",
        "            if target_paper_details:\n",
        "                print(f\"RAG: Retrieving for: {[p['index'] for p in target_paper_details]}\")\n",
        "                for details in target_paper_details:\n",
        "                    fpath = details['filepath']; paper_title = details['title']; paper_index = details['index']\n",
        "                    docs = vs.similarity_search(query=paper_title, k=retrieval_k, filter={'source': fpath})\n",
        "                    if not docs: docs = vs.similarity_search(query=question, k=retrieval_k, filter={'source': fpath})\n",
        "                    if not docs:\n",
        "                        generic_query = \"research paper content\"\n",
        "                        docs = vs.similarity_search(query=generic_query, k=retrieval_k, filter={'source': fpath})\n",
        "                        if not docs:\n",
        "                             retrieval_failure_reason = f\"Could not retrieve any text chunks for Paper {paper_index}. The PDF might be image-based or have failed processing during RAG indexing (check console logs).\"\n",
        "                             print(f\"    -> WARNING: {retrieval_failure_reason}\")\n",
        "                    relevant_docs.extend(docs)\n",
        "            else:\n",
        "                print(\"RAG: Retrieving generally using question.\")\n",
        "                relevant_docs = vs.similarity_search(question, k=retrieval_k + 2)\n",
        "            if relevant_docs: retrieval_successful = True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå RAG: Error during retrieval: {e}\"); history[-1] = (question, f\"‚ö†Ô∏è Retrieval error: {e}\"); yield history, history, current_app_state; return\n",
        "\n",
        "        # --- Generate Response using QA Chain ---\n",
        "        llm_answer = retrieval_failure_reason if retrieval_failure_reason else \"Sorry, RAG couldn't find relevant information for your query in the available document excerpts.\"\n",
        "        is_llm_refusal = True # Assume refusal by default\n",
        "        source_docs_used = []\n",
        "\n",
        "        if retrieval_successful:\n",
        "            try:\n",
        "                unique_docs = []; seen_doc_content = set()\n",
        "                for doc in relevant_docs:\n",
        "                    if doc.page_content and len(doc.page_content.strip()) > 10:\n",
        "                         if doc.page_content not in seen_doc_content:\n",
        "                            unique_docs.append(doc); seen_doc_content.add(doc.page_content)\n",
        "\n",
        "                if unique_docs:\n",
        "                    print(f\"RAG: Running QA chain with {len(unique_docs)} unique chunks.\")\n",
        "                    qa_prompt = PromptTemplate(template=prompt_template_str, input_variables=[\"context\", \"question\"])\n",
        "                    document_prompt_template = \"\"\"**Source: '{title}' - page {page}**\n",
        "                    {page_content}\"\"\"\n",
        "                    document_prompt = PromptTemplate(input_variables=[\"page_content\", \"title\", \"page\"], template=document_prompt_template)\n",
        "\n",
        "                    # --- *** Modify Question for Single-Paper Summaries *** ---\n",
        "                    effective_question = question # Default to original user question\n",
        "                    is_index_query_single = bool(re.search(r'paper\\s+\\d+', question, re.IGNORECASE)) and not bool(re.search(r'paper titled', question, re.IGNORECASE)) and len(target_paper_details) == 1\n",
        "                    is_summary_request = \"summarize\" in question.lower() or \"summary\" in question.lower()\n",
        "\n",
        "                    if is_index_query_single and is_summary_request:\n",
        "                        paper_title = target_paper_details[0]['title']\n",
        "                        effective_question = f\"Provide a concise summary of the key points, findings, and conclusions presented in the provided context excerpts from the paper titled '{paper_title}'.\"\n",
        "                        print(f\"RAG: Modified question for LLM: {effective_question}\")\n",
        "                    # --- *** End Modification *** ---\n",
        "\n",
        "                    # Execute QA chain\n",
        "                    qa_chain = load_qa_chain(llm=llm, chain_type=\"stuff\", prompt=qa_prompt, document_prompt=document_prompt, verbose=False)\n",
        "                    # Use the potentially modified question here\n",
        "                    result = qa_chain({\"input_documents\": unique_docs, \"question\": effective_question}, return_only_outputs=False)\n",
        "                    generated_text = result['output_text'].strip()\n",
        "\n",
        "                    # --- Stricter Refusal Check ---\n",
        "                    refusal_phrases = [ # Keep the comprehensive list\n",
        "                        \"provided context does not contain\", \"information is not available\",\n",
        "                        \"couldn't find information\", \"do not have information\",\n",
        "                        \"cannot answer based on the context\", \"context provided does not mention\",\n",
        "                        \"based on the text provided\", \"relevant information was not found\",\n",
        "                        \"i cannot provide a summary\", \"unable to summarize\", \"does not seem to contain\",\n",
        "                        \"no specific information\", \"no details provided\"\n",
        "                    ]\n",
        "                    is_llm_refusal = any(phrase in generated_text.lower() for phrase in refusal_phrases) or \\\n",
        "                                     (len(generated_text.split()) < 15 and (\"sorry\" in generated_text.lower() or \"unable\" in generated_text.lower()))\n",
        "\n",
        "                    llm_answer = generated_text # Store the LLM's actual output\n",
        "\n",
        "                    if not is_llm_refusal:\n",
        "                         source_docs_used = result.get('input_documents', []) # Store sources ONLY if not a refusal\n",
        "                         print(f\"RAG: LLM generated a valid answer.\")\n",
        "                    else:\n",
        "                         print(f\"RAG: LLM generated a refusal or very short answer: '{generated_text[:100]}...'\")\n",
        "                    # --- End Stricter Refusal Check ---\n",
        "\n",
        "                else: print(\"RAG: No unique, non-empty documents after filtering.\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå RAG: Error running QA chain: {e}\"); import traceback; traceback.print_exc();\n",
        "                error_type = type(e).__name__; history[-1] = (question, f\"‚ö†Ô∏è Answer generation error: {error_type}\"); yield history, history, current_app_state; return\n",
        "        else:\n",
        "            print(f\"RAG: Retrieval did not find any relevant documents. Failure Reason: {retrieval_failure_reason if retrieval_failure_reason else 'N/A'}\")\n",
        "            # llm_answer already holds the failure message, is_llm_refusal remains True\n",
        "\n",
        "        # --- Add Title Prefix for Index-Based Queries ---\n",
        "        response_prefix = \"\"\n",
        "        is_index_query = bool(re.search(r'paper\\s+\\d+', question, re.IGNORECASE)) and not bool(re.search(r'paper titled', question, re.IGNORECASE))\n",
        "        # Add prefix only if it was an index query AND it was NOT a refusal\n",
        "        if is_index_query and target_paper_details and not is_llm_refusal:\n",
        "             if len(target_paper_details) == 1:\n",
        "                  p = target_paper_details[0]; response_prefix = f\"**Summary for Paper {p['index']} ('{p['title']}')**: \\n\\n\"\n",
        "             elif len(target_paper_details) > 1:\n",
        "                  titles_str = \" vs \".join([f\"Paper {p['index']} ('{p['title']}')\" for p in target_paper_details]); response_prefix = f\"**Comparison of {titles_str}**: \\n\\n\"\n",
        "        final_answer_text = response_prefix + llm_answer\n",
        "\n",
        "        # --- Source Attribution (Conditional on NOT being a refusal) ---\n",
        "        if not is_llm_refusal and source_docs_used:\n",
        "            cited_sources_info = []; seen_source_keys = set()\n",
        "            for doc in source_docs_used:\n",
        "                meta = doc.metadata; title = meta.get('title', 'Unknown')\n",
        "                page_num_0_indexed = meta.get('page', -1)\n",
        "                source_key = f\"{title}_{page_num_0_indexed}\"\n",
        "                if source_key not in seen_source_keys:\n",
        "                    page_str = f\", page ~{page_num_0_indexed + 1}\" if page_num_0_indexed != -1 else \"\"\n",
        "                    cited_sources_info.append(f\"'{title}'{page_str}\"); seen_source_keys.add(source_key)\n",
        "            if cited_sources_info:\n",
        "                 cited_sources_info.sort(); sources_text = \"\\n\\n*Sources (RAG):* \\n\" + \"\\n\".join([f\"- {s}\" for s in cited_sources_info])\n",
        "                 final_answer_text += sources_text\n",
        "\n",
        "        history[-1] = (question, final_answer_text)\n",
        "\n",
        "        yield history, history, current_app_state\n",
        "\n",
        "    # Connect Ask button and Enter key in textbox to the chat handler\n",
        "    ask_button.click(handle_chat_message, inputs=[question_input, chatbot_output, app_state], outputs=[chatbot_output, chatbot_output, app_state]).then(lambda: gr.update(value=\"\"), outputs=[question_input])\n",
        "    question_input.submit(handle_chat_message, inputs=[question_input, chatbot_output, app_state], outputs=[chatbot_output, chatbot_output, app_state]).then(lambda: gr.update(value=\"\"), outputs=[question_input])\n",
        "\n",
        "    # 3. Clear Chat Button Logic\n",
        "    def clear_chat_history(): return [], [] # Simple function to return empty lists\n",
        "    clear_chat_button.click(clear_chat_history, inputs=None, outputs=[chatbot_output, chatbot_output])\n",
        "\n",
        "\n",
        "# --- 5. Launch the Gradio App ---\n",
        "print(\"\\nLaunching Gradio App...\")\n",
        "# queue() allows multiple users/requests concurrently\n",
        "# share=True creates a public link needed for Colab\n",
        "# debug=True provides helpful logs in the Colab output console\n",
        "demo.queue().launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oKiS-pOXIATx",
        "outputId": "d898f847-6732-4571-b72e-9a25d2787944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-126bb6cdcb19>:466: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot_output = gr.Chatbot(label=\"Conversation\", height=450, bubble_full_width=False, avatar_images=None)\n",
            "<ipython-input-10-126bb6cdcb19>:466: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot_output = gr.Chatbot(label=\"Conversation\", height=450, bubble_full_width=False, avatar_images=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Launching Gradio App...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d3c6791eb52b8b82a5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d3c6791eb52b8b82a5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/blocks.py:1925: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  state[block._id] = block.__class__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching papers for topic: 'AI research assistant', sorting by: Relevance, max results: 25\n",
            "Cleared previous downloads, references, and RAG index.\n",
            "Found 25 papers. Downloading...\n",
            "[1/25] Downloading 'Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in AI-assisted Decision Making' to 2401.05840v1_Decoding_AI's_Nudge_A_Unified_Framework_to_Predict_Human_Behavior_in_AI-assisted_Decisi.pdf...\n",
            "[1/25] Download complete.\n",
            "[2/25] Downloading 'AI-Empowered Human Research Integrating Brain Science and Social Sciences Insights' to 2411.12761v1_AI-Empowered_Human_Research_Integrating_Brain_Science_and_Social_Sciences_Insights.pdf...\n",
            "[2/25] Download complete.\n",
            "[3/25] Downloading 'AI empowering research: 10 ways how science can benefit from AI' to 2307.10265v1_AI_empowering_research_10_ways_how_science_can_benefit_from_AI.pdf...\n",
            "[3/25] Download complete.\n",
            "[4/25] Downloading 'Human-AI Collaborative Game Testing with Vision Language Models' to 2501.11782v1_Human-AI_Collaborative_Game_Testing_with_Vision_Language_Models.pdf...\n",
            "[4/25] Download complete.\n",
            "[5/25] Downloading 'Measuring Human Contribution in AI-Assisted Content Generation' to 2408.14792v2_Measuring_Human_Contribution_in_AI-Assisted_Content_Generation.pdf...\n",
            "[5/25] Download complete.\n",
            "[6/25] Downloading 'Envisioning the Next-Generation AI Coding Assistants: Insights & Proposals' to 2403.14592v1_Envisioning_the_Next-Generation_AI_Coding_Assistants_Insights_&_Proposals.pdf...\n",
            "[6/25] Download complete.\n",
            "[7/25] Downloading 'Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization' to 2112.12387v1_Human-AI_Collaboration_for_UX_Evaluation_Effects_of_Explanation_and_Synchronization.pdf...\n",
            "[7/25] Download complete.\n",
            "[8/25] Downloading 'Reporting Risks in AI-based Assistive Technology Research: A Systematic Review' to 2407.12035v2_Reporting_Risks_in_AI-based_Assistive_Technology_Research_A_Systematic_Review.pdf...\n",
            "[8/25] Download complete.\n",
            "[9/25] Downloading 'How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in Software Engineering' to 2501.08774v2_How_Developers_Interact_with_AI_A_Taxonomy_of_Human-AI_Collaboration_in_Software_Engine.pdf...\n",
            "[9/25] Download complete.\n",
            "[10/25] Downloading 'Collective Attention in Human-AI Teams' to 2407.17489v1_Collective_Attention_in_Human-AI_Teams.pdf...\n",
            "[10/25] Download complete.\n",
            "[11/25] Downloading 'Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making' to 2001.02114v1_Effect_of_Confidence_and_Explanation_on_Accuracy_and_Trust_Calibration_in_AI-Assisted_D.pdf...\n",
            "[11/25] Download complete.\n",
            "[12/25] Downloading 'Towards AI-assisted Academic Writing' to 2503.13771v1_Towards_AI-assisted_Academic_Writing.pdf...\n",
            "[12/25] Download complete.\n",
            "[13/25] Downloading 'How Problematic Writer-AI Interactions (Rather than Problematic AI) Hinder Writers' Idea Generation' to 2503.11915v1_How_Problematic_Writer-AI_Interactions_(Rather_than_Problematic_AI)_Hinder_Writers'_Ide.pdf...\n",
            "[13/25] Download complete.\n",
            "[14/25] Downloading 'AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals' to 2412.15444v1_AI-Enhanced_Sensemaking_Exploring_the_Design_of_a_Generative_AI-Based_Assistant_to_Supp.pdf...\n",
            "[14/25] Download complete.\n",
            "[15/25] Downloading 'Towards Decoding Developer Cognition in the Age of AI Assistants' to 2501.02684v1_Towards_Decoding_Developer_Cognition_in_the_Age_of_AI_Assistants.pdf...\n",
            "[15/25] Download complete.\n",
            "[16/25] Downloading 'AI-based Programming Assistants for Privacy-related Code Generation: The Developers' Experience' to 2503.03988v1_AI-based_Programming_Assistants_for_Privacy-related_Code_Generation_The_Developers'_Exp.pdf...\n",
            "[16/25] Download complete.\n",
            "[17/25] Downloading 'Interactive Example-based Explanations to Improve Health Professionals' Onboarding with AI for Human-AI Collaborative Decision Making' to 2409.15814v1_Interactive_Example-based_Explanations_to_Improve_Health_Professionals'_Onboarding_with.pdf...\n",
            "[17/25] Download complete.\n",
            "[18/25] Downloading 'Resource Optimization in UAV-assisted IoT Networks: The Role of Generative AI' to 2405.03863v1_Resource_Optimization_in_UAV-assisted_IoT_Networks_The_Role_of_Generative_AI.pdf...\n",
            "[18/25] Download complete.\n",
            "[19/25] Downloading 'Struggle First, Prompt Later: How Task Complexity Shapes Learning with GenAI-Assisted Pretesting' to 2504.10249v1_Struggle_First,_Prompt_Later_How_Task_Complexity_Shapes_Learning_with_GenAI-Assisted_Pr.pdf...\n",
            "[19/25] Download complete.\n",
            "[20/25] Downloading 'Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making' to 2308.04375v1_Understanding_the_Effect_of_Counterfactual_Explanations_on_Trust_and_Reliance_on_AI_for.pdf...\n",
            "[20/25] Download complete.\n",
            "[21/25] Downloading 'The AI Double Standard: Humans Judge All AIs for the Actions of One' to 2412.06040v1_The_AI_Double_Standard_Humans_Judge_All_AIs_for_the_Actions_of_One.pdf...\n",
            "[21/25] Download complete.\n",
            "[22/25] Downloading 'Formative Study for AI-assisted Data Visualization' to 2409.06892v1_Formative_Study_for_AI-assisted_Data_Visualization.pdf...\n",
            "[22/25] Download complete.\n",
            "[23/25] Downloading 'Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution' to 2504.13847v1_Interview_AI-ssistant_Designing_for_Real-Time_Human-AI_Collaboration_in_Interview_Prepa.pdf...\n",
            "[23/25] Download complete.\n",
            "[24/25] Downloading 'Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted Decision-Making' to 2504.03207v1_Augmenting_Human_Cognition_With_Generative_AI_Lessons_From_AI-Assisted_Decision-Making.pdf...\n",
            "[24/25] Download complete.\n",
            "[25/25] Downloading 'Notes on a Path to AI Assistance in Mathematical Reasoning' to 2310.02896v1_Notes_on_a_Path_to_AI_Assistance_in_Mathematical_Reasoning.pdf...\n",
            "[25/25] Download complete.\n",
            "Downloads finished. References saved in 'arxiv_references/references_AI_research_assistant_20250427_1528.txt'.\n",
            "Loading PDFs for RAG indexing (using Langchain PyPDFLoader)...\n",
            " -> RAG Indexing: Attempting to load Paper 1 ('Decoding AI's Nudge: A Unified Framework...') from 2401.05840v1_Decoding_AI's_Nudge_A_Unified_Framework_to_Predict_Human_Behavior_in_AI-assisted_Decisi.pdf\n",
            "    -> Loaded 12 pages for Paper 1.\n",
            " -> RAG Indexing: Attempting to load Paper 2 ('AI-Empowered Human Research Integrating ...') from 2411.12761v1_AI-Empowered_Human_Research_Integrating_Brain_Science_and_Social_Sciences_Insights.pdf\n",
            "    -> Loaded 10 pages for Paper 2.\n",
            " -> RAG Indexing: Attempting to load Paper 3 ('AI empowering research: 10 ways how scie...') from 2307.10265v1_AI_empowering_research_10_ways_how_science_can_benefit_from_AI.pdf\n",
            "    -> Loaded 10 pages for Paper 3.\n",
            " -> RAG Indexing: Attempting to load Paper 4 ('Human-AI Collaborative Game Testing with...') from 2501.11782v1_Human-AI_Collaborative_Game_Testing_with_Vision_Language_Models.pdf\n",
            "    -> Loaded 10 pages for Paper 4.\n",
            " -> RAG Indexing: Attempting to load Paper 5 ('Measuring Human Contribution in AI-Assis...') from 2408.14792v2_Measuring_Human_Contribution_in_AI-Assisted_Content_Generation.pdf\n",
            "    -> Loaded 16 pages for Paper 5.\n",
            " -> RAG Indexing: Attempting to load Paper 6 ('Envisioning the Next-Generation AI Codin...') from 2403.14592v1_Envisioning_the_Next-Generation_AI_Coding_Assistants_Insights_&_Proposals.pdf\n",
            "    -> Loaded 3 pages for Paper 6.\n",
            " -> RAG Indexing: Attempting to load Paper 7 ('Human-AI Collaboration for UX Evaluation...') from 2112.12387v1_Human-AI_Collaboration_for_UX_Evaluation_Effects_of_Explanation_and_Synchronization.pdf\n",
            "    -> Loaded 31 pages for Paper 7.\n",
            " -> RAG Indexing: Attempting to load Paper 8 ('Reporting Risks in AI-based Assistive Te...') from 2407.12035v2_Reporting_Risks_in_AI-based_Assistive_Technology_Research_A_Systematic_Review.pdf\n",
            "    -> Loaded 12 pages for Paper 8.\n",
            " -> RAG Indexing: Attempting to load Paper 9 ('How Developers Interact with AI: A Taxon...') from 2501.08774v2_How_Developers_Interact_with_AI_A_Taxonomy_of_Human-AI_Collaboration_in_Software_Engine.pdf\n",
            "    -> Loaded 5 pages for Paper 9.\n",
            " -> RAG Indexing: Attempting to load Paper 10 ('Collective Attention in Human-AI Teams...') from 2407.17489v1_Collective_Attention_in_Human-AI_Teams.pdf\n",
            "    -> Loaded 21 pages for Paper 10.\n",
            " -> RAG Indexing: Attempting to load Paper 11 ('Effect of Confidence and Explanation on ...') from 2001.02114v1_Effect_of_Confidence_and_Explanation_on_Accuracy_and_Trust_Calibration_in_AI-Assisted_D.pdf\n",
            "    -> Loaded 11 pages for Paper 11.\n",
            " -> RAG Indexing: Attempting to load Paper 12 ('Towards AI-assisted Academic Writing...') from 2503.13771v1_Towards_AI-assisted_Academic_Writing.pdf\n",
            "    -> Loaded 15 pages for Paper 12.\n",
            " -> RAG Indexing: Attempting to load Paper 13 ('How Problematic Writer-AI Interactions (...') from 2503.11915v1_How_Problematic_Writer-AI_Interactions_(Rather_than_Problematic_AI)_Hinder_Writers'_Ide.pdf\n",
            "    -> Loaded 7 pages for Paper 13.\n",
            " -> RAG Indexing: Attempting to load Paper 14 ('AI-Enhanced Sensemaking: Exploring the D...') from 2412.15444v1_AI-Enhanced_Sensemaking_Exploring_the_Design_of_a_Generative_AI-Based_Assistant_to_Supp.pdf\n",
            "    -> Loaded 28 pages for Paper 14.\n",
            " -> RAG Indexing: Attempting to load Paper 15 ('Towards Decoding Developer Cognition in ...') from 2501.02684v1_Towards_Decoding_Developer_Cognition_in_the_Age_of_AI_Assistants.pdf\n",
            "    -> Loaded 7 pages for Paper 15.\n",
            " -> RAG Indexing: Attempting to load Paper 16 ('AI-based Programming Assistants for Priv...') from 2503.03988v1_AI-based_Programming_Assistants_for_Privacy-related_Code_Generation_The_Developers'_Exp.pdf\n",
            "    -> Loaded 10 pages for Paper 16.\n",
            " -> RAG Indexing: Attempting to load Paper 17 ('Interactive Example-based Explanations t...') from 2409.15814v1_Interactive_Example-based_Explanations_to_Improve_Health_Professionals'_Onboarding_with.pdf\n",
            "    -> Loaded 8 pages for Paper 17.\n",
            " -> RAG Indexing: Attempting to load Paper 18 ('Resource Optimization in UAV-assisted Io...') from 2405.03863v1_Resource_Optimization_in_UAV-assisted_IoT_Networks_The_Role_of_Generative_AI.pdf\n",
            "    -> Loaded 8 pages for Paper 18.\n",
            " -> RAG Indexing: Attempting to load Paper 19 ('Struggle First, Prompt Later: How Task C...') from 2504.10249v1_Struggle_First,_Prompt_Later_How_Task_Complexity_Shapes_Learning_with_GenAI-Assisted_Pr.pdf\n",
            "    -> Loaded 17 pages for Paper 19.\n",
            " -> RAG Indexing: Attempting to load Paper 20 ('Understanding the Effect of Counterfactu...') from 2308.04375v1_Understanding_the_Effect_of_Counterfactual_Explanations_on_Trust_and_Reliance_on_AI_for.pdf\n",
            "    -> Loaded 22 pages for Paper 20.\n",
            " -> RAG Indexing: Attempting to load Paper 21 ('The AI Double Standard: Humans Judge All...') from 2412.06040v1_The_AI_Double_Standard_Humans_Judge_All_AIs_for_the_Actions_of_One.pdf\n",
            "    -> Loaded 24 pages for Paper 21.\n",
            " -> RAG Indexing: Attempting to load Paper 22 ('Formative Study for AI-assisted Data Vis...') from 2409.06892v1_Formative_Study_for_AI-assisted_Data_Visualization.pdf\n",
            "    -> Loaded 11 pages for Paper 22.\n",
            " -> RAG Indexing: Attempting to load Paper 23 ('Interview AI-ssistant: Designing for Rea...') from 2504.13847v1_Interview_AI-ssistant_Designing_for_Real-Time_Human-AI_Collaboration_in_Interview_Prepa.pdf\n",
            "    -> Loaded 4 pages for Paper 23.\n",
            " -> RAG Indexing: Attempting to load Paper 24 ('Augmenting Human Cognition With Generati...') from 2504.03207v1_Augmenting_Human_Cognition_With_Generative_AI_Lessons_From_AI-Assisted_Decision-Making.pdf\n",
            "    -> Loaded 5 pages for Paper 24.\n",
            " -> RAG Indexing: Attempting to load Paper 25 ('Notes on a Path to AI Assistance in Math...') from 2310.02896v1_Notes_on_a_Path_to_AI_Assistance_in_Mathematical_Reasoning.pdf\n",
            "    -> Loaded 7 pages for Paper 25.\n",
            "RAG Indexing: Successfully loaded pages from 25/25 valid PDFs using PyPDFLoader.\n",
            "RAG Indexing: Splitting text for 314 pages...\n",
            "RAG Indexing: Chunks created per paper index:\n",
            "  - Paper 1: 51 chunks\n",
            "  - Paper 2: 40 chunks\n",
            "  - Paper 3: 27 chunks\n",
            "  - Paper 4: 47 chunks\n",
            "  - Paper 5: 52 chunks\n",
            "  - Paper 6: 13 chunks\n",
            "  - Paper 7: 104 chunks\n",
            "  - Paper 8: 34 chunks\n",
            "  - Paper 9: 22 chunks\n",
            "  - Paper 10: 78 chunks\n",
            "  - Paper 11: 52 chunks\n",
            "  - Paper 12: 44 chunks\n",
            "  - Paper 13: 29 chunks\n",
            "  - Paper 14: 94 chunks\n",
            "  - Paper 15: 32 chunks\n",
            "  - Paper 16: 35 chunks\n",
            "  - Paper 17: 41 chunks\n",
            "  - Paper 18: 35 chunks\n",
            "  - Paper 19: 62 chunks\n",
            "  - Paper 20: 65 chunks\n",
            "  - Paper 21: 80 chunks\n",
            "  - Paper 22: 53 chunks\n",
            "  - Paper 23: 22 chunks\n",
            "  - Paper 24: 22 chunks\n",
            "  - Paper 25: 14 chunks\n",
            "RAG Indexing: Split into 1148 total chunks. Creating OpenAI embeddings...\n",
            "RAG Indexing: Initializing FAISS vector store...\n",
            "RAG Indexing: FAISS index created successfully with 1148 vectors.\n",
            "Setting up LLM (gpt-3.5-turbo) and QA Prompt Template String...\n",
            "LLM and Prompt Template String setup complete.\n",
            "TF-IDF Phase: Extracting full text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/blocks.py:1925: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  state[block._id] = block.__class__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Phase: Successfully extracted text from 25/25 papers.\n",
            "Fitting TF-IDF Vectorizer...\n",
            "TF-IDF Vocabulary size: 9368\n",
            "Calculating TF-IDF Paper-to-Paper Similarity Matrix...\n",
            " -> Calculated Paper-to-Paper Similarity Matrix (TF-IDF).\n",
            "Generating heatmap: Paper-to-Paper Similarity (TF-IDF)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/blocks.py:1925: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  state[block._id] = block.__class__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed query, identified target paper(s): [1]\n",
            "RAG: Retrieving for: [1]\n",
            "RAG: Running QA chain with 5 unique chunks.\n",
            "RAG: Modified question for LLM: Provide a concise summary of the key points, findings, and conclusions presented in the provided context excerpts from the paper titled 'Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in AI-assisted Decision Making'.\n",
            "RAG: LLM generated a valid answer.\n",
            "Parsed query, identified target paper(s): [10]\n",
            "RAG: Retrieving for: [10]\n",
            "RAG: Running QA chain with 5 unique chunks.\n",
            "RAG: Modified question for LLM: Provide a concise summary of the key points, findings, and conclusions presented in the provided context excerpts from the paper titled 'Collective Attention in Human-AI Teams'.\n",
            "RAG: LLM generated a valid answer.\n",
            "Parsed query, identified target paper(s): [25]\n",
            "RAG: Retrieving for: [25]\n",
            "RAG: Running QA chain with 5 unique chunks.\n",
            "RAG: Modified question for LLM: Provide a concise summary of the key points, findings, and conclusions presented in the provided context excerpts from the paper titled 'Notes on a Path to AI Assistance in Mathematical Reasoning'.\n",
            "RAG: LLM generated a valid answer.\n",
            "Parsed query, identified target paper(s): [17, 20]\n",
            "RAG: Retrieving for: [17, 20]\n",
            "RAG: Running QA chain with 10 unique chunks.\n",
            "RAG: LLM generated a valid answer.\n",
            "Parsed query, identified target paper(s): [9, 21]\n",
            "RAG: Retrieving for: [9, 21]\n",
            "RAG: Running QA chain with 10 unique chunks.\n",
            "RAG: LLM generated a valid answer.\n",
            "Parsed query, no specific paper references identified.\n",
            "RAG: Retrieving generally using question.\n",
            "RAG: Running QA chain with 7 unique chunks.\n",
            "RAG: LLM generated a valid answer.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d3c6791eb52b8b82a5.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "52LjakBpIAWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O7zLOFWMIAYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yHGlCYwdIAa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1W6XAWeqIAdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVKKfUxhIAfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kyv_H5YjMdRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Ie_7gx7MdTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "omMCikHt-hk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0hZ8cOav-hnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BUNsp2n-hpe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}